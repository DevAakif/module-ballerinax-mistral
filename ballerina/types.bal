// AUTO-GENERATED FILE. DO NOT MODIFY.
// This file is auto-generated by the Ballerina OpenAPI tool.

import ballerina/constraint;
import ballerina/http;

public type ToolCall record {|
    FunctionCall 'function;
    int index = 0;
    string id = "null";
    ToolTypes 'type?;
|};

public type JsonSchema record {|
    record {} schema;
    string name;
    string? description?;
    boolean strict = false;
|};

public type ChatCompletionChoice record {
    "stop"|"length"|"model_length"|"error"|"tool_calls" finish_reason;
    int index;
    AssistantMessage message;
};

public type OCRResponse record {|
    # List of OCR info for pages
    OCRPageObject[] pages;
    # The model used to generate the OCR
    string model;
    OCRUsageInfo usage_info;
|};

public type ArchiveFTModelOut record {
    boolean archived = true;
    string id;
    "model" 'object = "model";
};

public type EventOut record {
    record {}? data?;
    # The name of the event
    string name;
    # The UNIX timestamp (in seconds) of the event
    int created_at;
};

public type GithubRepositoryIn record {
    string owner;
    string? ref?;
    string name;
    @constraint:Number {minValueExclusive: 0}
    decimal weight = 1;
    "github" 'type = "github";
    string token;
};

public type FileSchema record {
    # The name of the uploaded file
    string filename;
    FilePurpose purpose;
    # The size of the file, in bytes
    int bytes;
    # The UNIX timestamp (in seconds) of the event
    int created_at;
    # The unique identifier of the file
    string id;
    Source 'source;
    SampleType sample_type;
    int? num_lines?;
    # The object type, which is always "file"
    string 'object;
};

# The fine-tuning hyperparameter settings used in a fine-tune job
public type TrainingParametersIn record {
    decimal? fim_ratio = 0.9;
    # (Advanced Usage) Weight decay adds a term to the loss function that is proportional to the sum of the squared weights. This term reduces the magnitude of the weights and prevents them from growing too large
    decimal? weight_decay = 0.1;
    # The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset
    int? training_steps?;
    # A parameter describing how much to adjust the pre-trained model's weights in response to the estimated error each time the weights are updated during the fine-tuning process
    @constraint:Number {minValue: 1.0E-8, maxValue: 1}
    decimal learning_rate = 0.00010;
    decimal? epochs?;
    int? seq_len?;
    # (Advanced Usage) A parameter that specifies the percentage of the total training steps at which the learning rate warm-up phase ends. During this phase, the learning rate gradually increases from a small value to the initial learning rate, helping to stabilize the training process and improve convergence. Similar to `pct_start` in [mistral-finetune](https://github.com/mistralai/mistral-finetune)
    decimal? warmup_fraction = 0.05;
};

# Provides a set of configurations for controlling the behaviours when communicating with a remote HTTP endpoint.
@display {label: "Connection Config"}
public type ConnectionConfig record {|
    # Configurations related to client authentication
    http:BearerTokenConfig auth;
    # The HTTP version understood by the client
    http:HttpVersion httpVersion = http:HTTP_2_0;
    # Configurations related to HTTP/1.x protocol
    ClientHttp1Settings http1Settings?;
    # Configurations related to HTTP/2 protocol
    http:ClientHttp2Settings http2Settings?;
    # The maximum time to wait (in seconds) for a response before closing the connection
    decimal timeout = 60;
    # The choice of setting `forwarded`/`x-forwarded` header
    string forwarded = "disable";
    # Configurations associated with request pooling
    http:PoolConfiguration poolConfig?;
    # HTTP caching related configurations
    http:CacheConfig cache?;
    # Specifies the way of handling compression (`accept-encoding`) header
    http:Compression compression = http:COMPRESSION_AUTO;
    # Configurations associated with the behaviour of the Circuit Breaker
    http:CircuitBreakerConfig circuitBreaker?;
    # Configurations associated with retrying
    http:RetryConfig retryConfig?;
    # Configurations associated with inbound response size limits
    http:ResponseLimitConfigs responseLimits?;
    # SSL/TLS-related options
    http:ClientSecureSocket secureSocket?;
    # Proxy server related options
    http:ProxyConfig proxy?;
    # Enables the inbound payload validation functionality which provided by the constraint package. Enabled by default
    boolean validation = true;
|};

public type FTModelCapabilitiesOut record {
    boolean completion_chat = true;
    boolean function_calling = false;
    boolean fine_tuning = false;
    boolean completion_fim = false;
};

public type ResponseFormat record {|
    JsonSchema json_schema?;
    ResponseFormats 'type?;
|};

public type UnarchiveFTModelOut record {
    boolean archived = false;
    string id;
    "model" 'object = "model";
};

public type TextChunk record {|
    string text;
    "text" 'type = "text";
|};

public type WandbIntegrationOut record {
    # A display name to set for the run. If not set, will use the job ID as the name
    string? name?;
    # The name of the project that the new run will be created under
    string project;
    "wandb" 'type = "wandb";
    string? run_name?;
};

public type ChatCompletionResponse record {
    *ChatCompletionResponseBase;
    *ChatCompletionResponse1;
};

public type FTModelOut record {
    boolean archived;
    FTModelCapabilitiesOut capabilities;
    string[] aliases = [];
    int max_context_length = 32768;
    int created;
    string root;
    string? name?;
    string? description?;
    string owned_by;
    string id;
    string job;
    "model" 'object = "model";
};

public type TrainingFile record {
    string file_id;
    @constraint:Number {minValueExclusive: 0}
    decimal weight = 1;
};

public type ModelListData BaseModelCard|FTModelCard;

public type ClassificationRequest record {|
    # Text to classify
    string|string[] input;
    # ID of the model to use
    string model;
|};

# Represents the Queries record for the operation: files_api_routes_list_files
public type Files_api_routes_list_filesQueries record {
    string? search?;
    FilePurpose purpose?;
    int page = 0;
    Source[]? 'source?;
    SampleType[]? sample_type?;
    int page_size = 100;
};

# Represents the Queries record for the operation: files_api_routes_get_signed_url
public type Files_api_routes_get_signed_urlQueries record {
    # Number of hours before the url becomes invalid. Defaults to 24h
    int expiry = 24;
};

# An object specifying the format that the model must output. Setting to `{ "type": "json_object" }` enables JSON mode, which guarantees the message the model generates is in JSON. When using JSON mode you MUST also instruct the model to produce JSON yourself with a system or a user message
public type ResponseFormats "text"|"json_object"|"json_schema";

# Represents the Queries record for the operation: jobs_api_routes_fine_tuning_create_fine_tuning_job
public type Jobs_api_routes_fine_tuning_create_fine_tuning_jobQueries record {
    # * If `true` the job is not spawned, instead the query returns a handful of useful metadata
    #   for the user to perform sanity checks (see `LegacyJobMetadataOut` response).
    # * Otherwise, the job is started and the query returns the job ID along with some of the
    #   input parameters (see `JobOut` response)
    boolean? dry_run?;
};

public type ContentChunk TextChunk|ImageURLChunk|DocumentURLChunk|ReferenceChunk;

public type MultiPartBodyParams record {
    # The File object (not file name) to be uploaded.
    #  To upload a file and specify a custom file name you should format your request as such:
    #  ```bash
    #  file=@path/to/your/file.jsonl;filename=custom_name.jsonl
    #  ```
    #  Otherwise, you can just keep the original file name:
    #  ```bash
    #  file=@path/to/your/file.jsonl
    #  ```
    record {byte[] fileContent; string fileName;} file;
    FilePurpose purpose?;
};

public type ChatCompletionResponseBase record {
    *ResponseBase;
    *ChatCompletionResponseBase1;
};

public type ApiEndpoint "/chat/completions"|"/embeddings"|"/fim/completions"|"/moderations"|"/chat/moderations";

public type BatchJobStatus "QUEUED"|"RUNNING"|"SUCCESS"|"FAILED"|"TIMEOUT_EXCEEDED"|"CANCELLATION_REQUESTED"|"CANCELLED";

public type FIMCompletionResponse record {
    *ChatCompletionResponse;
    string model?;
};

public type DeleteModelOut record {
    # The deletion status
    boolean deleted = true;
    # The ID of the deleted model
    string id;
    # The object type that was deleted
    string 'object = "model";
};

public type OCRUsageInfo record {|
    # Number of pages processed
    @constraint:Int {minValue: 0}
    int pages_processed;
    # Document size in bytes
    int? doc_size_bytes?;
|};

public type DetailedJobOut record {
    string job_type;
    JobMetadataOut metadata?;
    string? fine_tuned_model?;
    int created_at;
    CheckpointOut[] checkpoints = [];
    string? suffix?;
    boolean auto_start;
    string[] training_files;
    DetailedJobOutRepositories[] repositories = [];
    TrainingParameters hyperparameters;
    FineTuneableModel model;
    string id;
    int? trained_tokens?;
    int modified_at;
    DetailedJobOutIntegrations[]? integrations?;
    # Event items are created every time the status of a fine-tuning job changes. The timestamped list of all events is accessible here
    EventOut[] events = [];
    "QUEUED"|"STARTED"|"VALIDATING"|"VALIDATED"|"RUNNING"|"FAILED_VALIDATION"|"FAILED"|"SUCCESS"|"CANCELLED"|"CANCELLATION_REQUESTED" status;
    string[]? validation_files = [];
    "job" 'object = "job";
};

public type OCRPageDimensions record {|
    # Width of the image in pixels
    @constraint:Int {minValue: 0}
    int width;
    # Dots per inch of the page-image
    @constraint:Int {minValue: 0}
    int dpi;
    # Height of the image in pixels
    @constraint:Int {minValue: 0}
    int height;
|};

public type UpdateFTModelIn record {
    string? name?;
    string? description?;
};

public type ResponseBase record {
    UsageInfo usage?;
    string model?;
    string id?;
    string 'object?;
};

public type ClassificationResponse record {
    string model?;
    string id?;
    ClassificationObject[] results?;
};

public type FunctionCall record {|
    string name;
    record {}|string arguments;
|};

public type UsageInfo record {
    int completion_tokens;
    int prompt_tokens;
    int total_tokens;
};

# Metrics at the step number during the fine-tuning job. Use these metrics to assess if the training is going smoothly (loss should decrease, token accuracy should increase)
public type MetricOut record {
    decimal? valid_loss?;
    decimal? valid_mean_token_accuracy?;
    decimal? train_loss?;
};

public type UploadFileOut record {
    # The name of the uploaded file
    string filename;
    FilePurpose purpose;
    # The size of the file, in bytes
    int bytes;
    # The UNIX timestamp (in seconds) of the event
    int created_at;
    # The unique identifier of the file
    string id;
    Source 'source;
    SampleType sample_type;
    int? num_lines?;
    # The object type, which is always "file"
    string 'object;
};

public type BatchJobIn record {
    string[] input_files;
    ApiEndpoint endpoint;
    record {|string...;|}? metadata?;
    int timeout_hours = 24;
    string model;
};

public type BatchError record {
    int count = 1;
    string message;
};

public type JobsOut record {
    int total;
    JobOut[] data = [];
    "list" 'object = "list";
};

# Represents the Queries record for the operation: jobs_api_routes_fine_tuning_get_fine_tuning_jobs
public type Jobs_api_routes_fine_tuning_get_fine_tuning_jobsQueries record {
    # The Weights and Biases project to filter on. When set, the other results are not displayed
    string? wandb_project?;
    # The Weight and Biases run name to filter on. When set, the other results are not displayed
    string? wandb_name?;
    # The date/time to filter on. When set, the results for previous creation times are not displayed
    string? created_after?;
    # The model name used for fine-tuning to filter on. When set, the other results are not displayed
    string? model?;
    # The page number of the results to be returned
    int page = 0;
    # The model suffix to filter on. When set, the other results are not displayed
    string? suffix?;
    # When set, only return results for jobs created by the API caller. Other results are not displayed
    boolean created_by_me = false;
    # The number of items to return per page
    int page_size = 100;
    # The current job state to filter on. When set, the other results are not displayed
    "QUEUED"|"STARTED"|"VALIDATING"|"VALIDATED"|"RUNNING"|"FAILED_VALIDATION"|"FAILED"|"SUCCESS"|"CANCELLED"|"CANCELLATION_REQUESTED"? status?;
};

public type ReferenceChunk record {|
    int[] reference_ids;
    "reference" 'type = "reference";
|};

# Provides settings related to HTTP/1.x protocol.
public type ClientHttp1Settings record {|
    # Specifies whether to reuse a connection for multiple requests
    http:KeepAlive keepAlive = http:KEEPALIVE_AUTO;
    # The chunking behaviour of the request
    http:Chunking chunking = http:CHUNKING_AUTO;
    # Proxy server related options
    ProxyConfig proxy?;
|};

public type OCRImageObject record {|
    # X coordinate of bottom-right corner of the extracted image
    int? bottom_right_x;
    # Y coordinate of bottom-right corner of the extracted image
    int? bottom_right_y;
    # Base64 string of the extracted image
    string? image_base64?;
    # Y coordinate of top-left corner of the extracted image
    int? top_left_y;
    # Image ID for extracted image in a page
    string id;
    # X coordinate of top-left corner of the extracted image
    int? top_left_x;
|};

public type SampleType "pretrain"|"instruct"|"batch_request"|"batch_result"|"batch_error";

public type JobInRepositories GithubRepositoryIn;

public type OCRRequest record {|
    # Specific pages user wants to process in various formats: single number, range, or list of both. Starts from 0
    int[]? pages?;
    # Minimum height and width of image to extract
    int? image_min_size?;
    # Document to run OCR on
    DocumentURLChunk|ImageURLChunk document;
    # Include image URLs in response
    boolean? include_image_base64?;
    # Max images to extract
    int? image_limit?;
    string? model;
    string id?;
|};

# Represents the Queries record for the operation: jobs_api_routes_batch_get_batch_jobs
public type Jobs_api_routes_batch_get_batch_jobsQueries record {
    record {}? metadata?;
    string? created_after?;
    string? model?;
    int page = 0;
    boolean created_by_me = false;
    int page_size = 100;
    BatchJobStatus status?;
};

public type FilePurpose "fine-tune"|"batch";

public type LegacyJobMetadataOut record {
    # The total number of tokens in the training dataset
    int? data_tokens?;
    # The number of tokens consumed by one training step
    int? train_tokens_per_step?;
    # The cost of the fine-tuning job
    decimal? cost?;
    # The currency used for the fine-tuning job cost
    string? cost_currency?;
    int? estimated_start_time?;
    # The approximated time (in seconds) for the fine-tuning process to complete
    int? expected_duration_seconds?;
    boolean deprecated = true;
    string details;
    # The total number of tokens used during the fine-tuning process
    int? train_tokens?;
    # The number of complete passes through the entire training dataset
    decimal? epochs?;
    # The number of training steps to perform. A training step refers to a single update of the model weights during the fine-tuning process. This update is typically calculated using a batch of samples from the training dataset
    int? training_steps?;
    "job.metadata" 'object = "job.metadata";
};

# this restriction of `Function` is used to select a specific function to call
public type FunctionName record {|
    string name;
|};

public type DeleteFileOut record {
    # The deletion status
    boolean deleted;
    # The ID of the deleted file
    string id;
    # The object type that was deleted
    string 'object;
};

public type DocumentURLChunk record {|
    # The filename of the document
    string? document_name?;
    string 'type = "document_url";
    string document_url;
|};

public type RetrieveFileOut record {
    # The name of the uploaded file
    string filename;
    boolean deleted;
    FilePurpose purpose;
    # The size of the file, in bytes
    int bytes;
    # The UNIX timestamp (in seconds) of the event
    int created_at;
    # The unique identifier of the file
    string id;
    Source 'source;
    SampleType sample_type;
    int? num_lines?;
    # The object type, which is always "file"
    string 'object;
};

public type DetailedJobOutIntegrations WandbIntegrationOut;

public type ResponseRetrieveModelV1ModelsModelIdGet BaseModelCard|FTModelCard;

public type ToolMessage record {|
    "tool" role = "tool";
    string? tool_call_id?;
    string? name?;
    string|ContentChunk[]? content;
|};

# Extra fields for fine-tuned models
public type FTModelCard record {
    ModelCapabilities capabilities;
    string[] aliases = [];
    int created?;
    string? description?;
    string owned_by = "mistralai";
    string? deprecation?;
    "fine-tuned" 'type = "fine-tuned";
    boolean archived = false;
    int max_context_length = 32768;
    string root;
    string? name?;
    decimal? default_model_temperature?;
    string id;
    string job;
    string 'object = "model";
};

public type Tool record {|
    Function 'function;
    ToolTypes 'type?;
|};

public type BatchJobsOut record {
    int total;
    BatchJobOut[] data = [];
    "list" 'object = "list";
};

public type AssistantMessage record {|
    "assistant" role = "assistant";
    # Set this to `true` when adding an assistant message as prefix to condition the model response. The role of the prefix message is to force the model to start its answer by the content of the message
    boolean prefix = false;
    ToolCall[]? tool_calls?;
    string|ContentChunk[]? content?;
|};

public type Prediction record {|
    string 'type = "content";
    string content = "";
|};

public type GithubRepositoryOut record {
    string owner;
    string? ref?;
    string name;
    @constraint:Number {minValueExclusive: 0}
    decimal weight = 1;
    "github" 'type = "github";
    @constraint:String {maxLength: 40, minLength: 40}
    string commit_id;
};

public type ChatCompletionResponseBase1 record {
    int created?;
};

public type ImageURL record {|
    string? detail?;
    string url;
|};

public type ChatModerationRequest record {|
    boolean truncate_for_context_length = false;
    # Chat to classify
    (SystemMessage|UserMessage|AssistantMessage|ToolMessage)[]|(SystemMessage|UserMessage|AssistantMessage|ToolMessage)[][] input;
    string model;
|};

public type ModelCapabilities record {
    boolean completion_chat = true;
    boolean function_calling = true;
    boolean vision = false;
    boolean fine_tuning = false;
    boolean completion_fim = false;
};

public type EmbeddingResponseData record {
    int index?;
    decimal[] embedding?;
    string 'object?;
};

public type ListFilesOut record {
    int total;
    FileSchema[] data;
    string 'object;
};

public type ToolTypes "function";

public type OCRPageObject record {|
    # List of all extracted images in the page
    OCRImageObject[] images;
    # The markdown string response of the page
    string markdown;
    # The page index in a pdf document starting from 0
    @constraint:Int {minValue: 0}
    int index;
    OCRPageDimensions dimensions;
|};

public type JobInIntegrations WandbIntegration;

public type FileSignedURL record {
    string url;
};

# ToolChoice is either a ToolChoiceEnum or a ToolChoice
public type ToolChoice record {|
    FunctionName 'function;
    ToolTypes 'type?;
|};

# The name of the model to fine-tune
public type FineTuneableModel "open-mistral-7b"|"mistral-small-latest"|"codestral-latest"|"mistral-large-latest"|"open-mistral-nemo"|"ministral-3b-latest";

public type BaseModelCard record {
    ModelCapabilities capabilities;
    string[] aliases = [];
    int max_context_length = 32768;
    int created?;
    string? name?;
    decimal? default_model_temperature?;
    string? description?;
    string owned_by = "mistralai";
    string id;
    string? deprecation?;
    "base" 'type = "base";
    string 'object = "model";
};

public type WandbIntegration record {
    # The WandB API key to use for authentication
    @constraint:String {maxLength: 40, minLength: 40}
    string api_key;
    # A display name to set for the run. If not set, will use the job ID as the name
    string? name?;
    # The name of the project that the new run will be created under
    string project;
    "wandb" 'type = "wandb";
    string? run_name?;
};

public type FIMCompletionRequest record {|
    # Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both
    @constraint:Number {minValue: 0, maxValue: 1}
    decimal top_p = 1;
    # The seed to use for random sampling. If set, different calls will generate deterministic results
    int? random_seed?;
    # Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
    string|string[] stop?;
    # The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length
    int? max_tokens?;
    # Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON
    boolean 'stream = false;
    # What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value
    decimal? temperature?;
    # ID of the model to use. Only compatible for now with:
    #   - `codestral-2405`
    #   - `codestral-latest`
    string model;
    # Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`
    string? suffix = "";
    # The text/code to complete
    string prompt;
    # The minimum number of tokens to generate in the completion
    int? min_tokens?;
|};

public type DetailedJobOutRepositories GithubRepositoryOut;

public type AgentsCompletionRequestMessages SystemMessage|UserMessage|AssistantMessage|ToolMessage;

public type SystemMessage record {|
    "system" role = "system";
    string|TextChunk[] content;
|};

public type ChatCompletionResponse1 record {
    ChatCompletionChoice[] choices?;
};

public type AgentsCompletionRequest record {|
    # The seed to use for random sampling. If set, different calls will generate deterministic results
    int? random_seed?;
    # The ID of the agent to use for this completion
    string agent_id;
    # The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length
    int? max_tokens?;
    # presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative
    @constraint:Number {minValue: -2, maxValue: 2}
    decimal presence_penalty = 0;
    Tool[]? tools?;
    # Number of completions to return for each request, input tokens are only billed once
    int? n?;
    ResponseFormat response_format?;
    # frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition
    @constraint:Number {minValue: -2, maxValue: 2}
    decimal frequency_penalty = 0;
    # Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
    string|string[] stop?;
    # Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON
    boolean 'stream = false;
    Prediction prediction?;
    # The prompt(s) to generate completions for, encoded as a list of dict with role and content
    AgentsCompletionRequestMessages[] messages;
    ToolChoice|ToolChoiceEnum tool_choice = "auto";
|};

public type JobMetadataOut record {
    int? data_tokens?;
    int? train_tokens_per_step?;
    decimal? cost?;
    string? cost_currency?;
    int? estimated_start_time?;
    int? expected_duration_seconds?;
    int? train_tokens?;
};

public type UserMessage record {|
    "user" role = "user";
    string|ContentChunk[]? content;
|};

public type Source "upload"|"repository"|"mistral";

public type ChatCompletionRequest record {|
    # The seed to use for random sampling. If set, different calls will generate deterministic results
    int? random_seed?;
    # Whether to inject a safety prompt before all conversations
    boolean safe_prompt = false;
    # The maximum number of tokens to generate in the completion. The token count of your prompt plus `max_tokens` cannot exceed the model's context length
    int? max_tokens?;
    # presence_penalty determines how much the model penalizes the repetition of words or phrases. A higher presence penalty encourages the model to use a wider variety of words and phrases, making the output more diverse and creative
    @constraint:Number {minValue: -2, maxValue: 2}
    decimal presence_penalty = 0;
    Tool[]? tools?;
    # Number of completions to return for each request, input tokens are only billed once
    int? n?;
    # Nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or `temperature` but not both
    @constraint:Number {minValue: 0, maxValue: 1}
    decimal top_p = 1;
    ResponseFormat response_format?;
    # frequency_penalty penalizes the repetition of words based on their frequency in the generated text. A higher frequency penalty discourages the model from repeating words that have already appeared frequently in the output, promoting diversity and reducing repetition
    @constraint:Number {minValue: -2, maxValue: 2}
    decimal frequency_penalty = 0;
    # Stop generation if this token is detected. Or if one of these tokens is detected when providing an array
    string|string[] stop?;
    # Whether to stream back partial progress. If set, tokens will be sent as data-only server-side events as they become available, with the stream terminated by a data: [DONE] message. Otherwise, the server will hold the request open until the timeout or until completion, with the response containing the full result as JSON
    boolean 'stream = false;
    # What sampling temperature to use, we recommend between 0.0 and 0.7. Higher values like 0.7 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both. The default value varies depending on the model you are targeting. Call the `/models` endpoint to retrieve the appropriate value
    decimal? temperature?;
    Prediction prediction?;
    # The prompt(s) to generate completions for, encoded as a list of dict with role and content
    AgentsCompletionRequestMessages[] messages;
    ToolChoice|ToolChoiceEnum tool_choice = "auto";
    # ID of the model to use. You can use the [List Available Models](/api/#tag/models/operation/list_models_v1_models_get) API to see all of your available models, or see our [Model overview](/models) for model descriptions
    string model;
|};

public type ModelList record {
    ModelListData[] data?;
    string 'object = "list";
};

# {"type":"image_url","image_url":{"url":"data:image/png;base64,iVBORw0
public type ImageURLChunk record {|
    ImageURL|string image_url;
    "image_url" 'type = "image_url";
|};

public type Function record {|
    string name;
    string description = "";
    boolean strict = false;
    record {} parameters;
|};

public type ToolChoiceEnum "auto"|"none"|"any"|"required";

public type TrainingParameters record {
    decimal? fim_ratio = 0.9;
    decimal? weight_decay = 0.1;
    int? training_steps?;
    @constraint:Number {minValue: 1.0E-8, maxValue: 1}
    decimal learning_rate = 0.00010;
    decimal? epochs?;
    int? seq_len?;
    decimal? warmup_fraction = 0.05;
};

public type JobOut record {
    # The type of job (`FT` for fine-tuning)
    string job_type;
    JobMetadataOut metadata?;
    # The name of the fine-tuned model that is being created. The value will be `null` if the fine-tuning job is still running
    string? fine_tuned_model?;
    # The UNIX timestamp (in seconds) for when the fine-tuning job was created
    int created_at;
    # Optional text/code that adds more context for the model. When given a `prompt` and a `suffix` the model will fill what is between them. When `suffix` is not provided, the model will simply execute completion starting with `prompt`
    string? suffix?;
    boolean auto_start;
    # A list containing the IDs of uploaded files that contain training data
    string[] training_files;
    DetailedJobOutRepositories[] repositories = [];
    TrainingParameters hyperparameters;
    FineTuneableModel model;
    # The ID of the job
    string id;
    # Total number of tokens trained
    int? trained_tokens?;
    # The UNIX timestamp (in seconds) for when the fine-tuning job was last modified
    int modified_at;
    # A list of integrations enabled for your fine-tuning job
    DetailedJobOutIntegrations[]? integrations?;
    # The current status of the fine-tuning job
    "QUEUED"|"STARTED"|"VALIDATING"|"VALIDATED"|"RUNNING"|"FAILED_VALIDATION"|"FAILED"|"SUCCESS"|"CANCELLED"|"CANCELLATION_REQUESTED" status;
    # A list containing the IDs of uploaded files that contain validation data
    string[]? validation_files = [];
    # The object type of the fine-tuning job
    "job" 'object = "job";
};

public type JobIn record {
    TrainingFile[] training_files = [];
    @constraint:Array {maxLength: 50}
    JobInRepositories[] repositories = [];
    TrainingParametersIn hyperparameters;
    FineTuneableModel model;
    # A string that will be added to your fine-tuning model name. For example, a suffix of "my-great-model" would produce a model name like `ft:open-mistral-7b:my-great-model:xxx...`
    string? suffix?;
    # A list of integrations to enable for your fine-tuning job
    JobInIntegrations[]? integrations?;
    # A list containing the IDs of uploaded files that contain validation data. If you provide these files, the data is used to generate validation metrics periodically during fine-tuning. These metrics can be viewed in `checkpoints` when getting the status of a running fine-tuning job. The same data should not be present in both train and validation files
    string[]? validation_files?;
    # This field will be required in a future release
    boolean auto_start?;
};

# Proxy server configurations to be used with the HTTP client endpoint.
public type ProxyConfig record {|
    # Host name of the proxy server
    string host = "";
    # Proxy server port
    int port = 0;
    # Proxy server username
    string userName = "";
    # Proxy server password
    @display {label: "", kind: "password"}
    string password = "";
|};

public type EmbeddingRequest record {|
    # Text to embed
    string|string[] input;
    # ID of the model to use
    string model;
|};

public type Response JobOut|LegacyJobMetadataOut;

public type BatchJobOut record {
    int succeeded_requests;
    record {}? metadata?;
    int failed_requests;
    int created_at;
    string? output_file?;
    string? error_file?;
    string[] input_files;
    int? completed_at?;
    string endpoint;
    int completed_requests;
    int total_requests;
    int? started_at?;
    string model;
    string id;
    BatchError[] errors;
    "batch" 'object = "batch";
    BatchJobStatus status;
};

public type EmbeddingResponse record {
    *ResponseBase;
    EmbeddingResponseData[] data;
};

public type ClassificationObject record {
    # Classifier result
    record {||} category_scores?;
    # Classifier result thresholded
    record {|boolean...;|} categories?;
};

public type CheckpointOut record {
    # The step number that the checkpoint was created at
    int step_number;
    # The UNIX timestamp (in seconds) for when the checkpoint was created
    int created_at;
    MetricOut metrics;
};
